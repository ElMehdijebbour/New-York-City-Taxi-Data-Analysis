{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30573aee",
   "metadata": {},
   "source": [
    "# New York City Taxi Data Analysis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa844ba0",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Project Overview](#project-overview)\n",
    "2. [Data Source Description](#data-source-description)\n",
    "3. [Project Steps](#project-steps)\n",
    "    - [Initialize PySpark Session](#initialize-pyspark-session)\n",
    "    - [Data Loading](#data-loading)\n",
    "    - [Data Enrichment](#data-enrichment)\n",
    "    - [Data Cleaning](#data-cleaning)\n",
    "    - [Query 1: Utilization](#query1-utilization)\n",
    "    - [Query 2: Average Time to Next Fare](#query2-average-time)\n",
    "    - [Query 3: Intra-Borough Trips](#query3-intra-borough)\n",
    "    - [Query 4: Inter-Borough Trips](#query4-inter-borough)\n",
    "4. [Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096402bc",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "This project aims to analyze a dataset containing New York City taxi ride information. The main goal is to compute the utilization of each taxi, understand the time it takes for a taxi to find its next passenger based on the borough of drop-off, and understand the intra-borough and inter-borough trips.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3a5a2f",
   "metadata": {},
   "source": [
    "## Data Source Description\n",
    "\n",
    "Each row in the dataset represents a single taxi ride in CSV format. The dataset provides:\n",
    "\n",
    "* Unique ID for the taxi (license)\n",
    "* Pick-up location (latitude and longitude)\n",
    "* Pick-up time\n",
    "* Drop-off location (latitude and longitude)\n",
    "* Drop-off time \\\n",
    "Additionally, the dataset includes a .geojson file with the geographical boundaries of the various NYC boroughs to identify and associate each ride with a specific borough.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7471f8",
   "metadata": {},
   "source": [
    "## Project Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ad1eb5",
   "metadata": {},
   "source": [
    "### Initialize PySpark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c152404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Taxi Ride Data Analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .config(\"spark.ui.retainedStages\", 100) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5596bb87",
   "metadata": {},
   "source": [
    "### Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5ace3698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/21 23:28:28 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---------------+----------------+---------------+----------------+-----------------+----------------+\n",
      "|hack_license                    |pickup_datetime|pickup_longitude|pickup_latitude|dropoff_datetime|dropoff_longitude|dropoff_latitude|\n",
      "+--------------------------------+---------------+----------------+---------------+----------------+-----------------+----------------+\n",
      "|BA96DE419E711691B9445D6A6307C170|01-01-13 15:11 |-73.978165      |40.757977      |01-01-13 15:18  |-73.989838       |40.751171       |\n",
      "|9FD8F69F0804BDB5549F40E9DA1BE472|06-01-13 0:18  |-74.006683      |40.731781      |06-01-13 0:22   |-73.994499       |40.75066        |\n",
      "|9FD8F69F0804BDB5549F40E9DA1BE472|05-01-13 18:49 |-74.004707      |40.73777       |05-01-13 18:54  |-74.009834       |40.726002       |\n",
      "|51EE87E3205C985EF8431D850C786310|07-01-13 23:54 |-73.974602      |40.759945      |07-01-13 23:58  |-73.984734       |40.759388       |\n",
      "|51EE87E3205C985EF8431D850C786310|07-01-13 23:25 |-73.97625       |40.748528      |07-01-13 23:34  |-74.002586       |40.747868       |\n",
      "+--------------------------------+---------------+----------------+---------------+----------------+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Read the CSV file with schema inference and immediately select the necessary columns\n",
    "taxi_data_final = (spark.read.csv(\"Sample_NYC_Data.csv\", header=True, inferSchema=True)\n",
    "            .select(\"hack_license\", \n",
    "                    \"pickup_datetime\", \n",
    "                    \"pickup_longitude\", \n",
    "                    \"pickup_latitude\", \n",
    "                    \"dropoff_datetime\", \n",
    "                    \"dropoff_longitude\", \n",
    "                    \"dropoff_latitude\"))\n",
    "\n",
    "# Cache the data\n",
    "taxi_data_final.cache()\n",
    "\n",
    "\n",
    "# Show the filtered data\n",
    "taxi_data_final.limit(5).show(truncate=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30d2dd3a",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c08f7d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning: 99999 rows, 7 columns\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import from_unixtime, unix_timestamp\n",
    "\n",
    "# Shape before cleaning\n",
    "before_rows = taxi_data_final.count()\n",
    "before_cols = len(taxi_data_final.columns)\n",
    "print(f\"Before cleaning: {before_rows} rows, {before_cols} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "29ddf667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- pickup_timestamp: timestamp (nullable = true)\n",
      " |-- dropoff_timestamp: timestamp (nullable = true)\n",
      " |-- Duration: double (nullable = true)\n",
      "\n",
      "+--------------------------------+----------------+---------------+-----------------+----------------+-------------------+-------------------+--------+\n",
      "|hack_license                    |pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|pickup_timestamp   |dropoff_timestamp  |Duration|\n",
      "+--------------------------------+----------------+---------------+-----------------+----------------+-------------------+-------------------+--------+\n",
      "|BA96DE419E711691B9445D6A6307C170|-73.978165      |40.757977      |-73.989838       |40.751171       |2013-01-01 15:11:00|2013-01-01 15:18:00|7.0     |\n",
      "|9FD8F69F0804BDB5549F40E9DA1BE472|-74.006683      |40.731781      |-73.994499       |40.75066        |2013-01-06 00:18:00|2013-01-06 00:22:00|4.0     |\n",
      "|9FD8F69F0804BDB5549F40E9DA1BE472|-74.004707      |40.73777       |-74.009834       |40.726002       |2013-01-05 18:49:00|2013-01-05 18:54:00|5.0     |\n",
      "|51EE87E3205C985EF8431D850C786310|-73.974602      |40.759945      |-73.984734       |40.759388       |2013-01-07 23:54:00|2013-01-07 23:58:00|4.0     |\n",
      "|51EE87E3205C985EF8431D850C786310|-73.97625       |40.748528      |-74.002586       |40.747868       |2013-01-07 23:25:00|2013-01-07 23:34:00|9.0     |\n",
      "+--------------------------------+----------------+---------------+-----------------+----------------+-------------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import unix_timestamp, col\n",
    "\n",
    "# Assuming spark session is already created\n",
    "# spark = SparkSession.builder.appName(\"TaxiData\").getOrCreate()\n",
    "\n",
    "taxi_data_final = taxi_data_final\n",
    "\n",
    "# Convert the date columns to unix timestamps\n",
    "taxi_data_final = taxi_data_final.withColumn(\"pickup_timestamp\", unix_timestamp(\"pickup_datetime\", \"dd-MM-yy H:mm\").cast(\"timestamp\"))\n",
    "taxi_data_final = taxi_data_final.withColumn(\"dropoff_timestamp\", unix_timestamp(\"dropoff_datetime\", \"dd-MM-yy H:mm\").cast(\"timestamp\"))\n",
    "\n",
    "# Calculate the duration in minutes between pickup and dropoff\n",
    "taxi_data_final = taxi_data_final.withColumn(\"Duration\", (col(\"dropoff_timestamp\").cast(\"long\") - col(\"pickup_timestamp\").cast(\"long\")) / 60)\n",
    "\n",
    "# Remove the old date columns\n",
    "taxi_data_final = taxi_data_final.drop(\"pickup_datetime\", \"dropoff_datetime\")\n",
    "\n",
    "# Show the schema and the results\n",
    "taxi_data_final.printSchema()\n",
    "taxi_data_final.limit(5).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a42f1518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in hack_license: 0\n",
      "Number of null values in pickup_longitude: 0\n",
      "Number of null values in pickup_latitude: 0\n",
      "Number of null values in dropoff_longitude: 0\n",
      "Number of null values in dropoff_latitude: 0\n",
      "Number of null values in pickup_timestamp: 0\n",
      "Number of null values in dropoff_timestamp: 0\n",
      "Number of null values in Duration: 0\n"
     ]
    }
   ],
   "source": [
    "for col_name in taxi_data_final.columns:\n",
    "    null_count = taxi_data_final.filter(taxi_data_final[col_name].isNull()).count()\n",
    "    print(f\"Number of null values in {col_name}: {null_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7fb3b683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "taxi_data_final = taxi_data_final.filter(\n",
    "    # NYC bounds\n",
    "    (col(\"pickup_latitude\") >= 40.4774) & (col(\"pickup_latitude\") <= 40.9176) &\n",
    "    (col(\"pickup_longitude\") >= -74.2591) & (col(\"pickup_longitude\") <= -73.7004) &\n",
    "    (col(\"dropoff_latitude\") >= 40.4774) & (col(\"dropoff_latitude\") <= 40.9176) &\n",
    "    (col(\"dropoff_longitude\") >= -74.2591) & (col(\"dropoff_longitude\") <= -73.7004) &\n",
    "\n",
    "    # Ensure Duration is reasonable (assuming it exists and is in minutes)\n",
    "    (col(\"Duration\") > 0) & (col(\"Duration\") <= 4*60) &\n",
    "\n",
    "    # Filter out zeros for latitudes and longitudes\n",
    "    (col(\"pickup_longitude\") != 0) & \n",
    "    (col(\"pickup_latitude\") != 0) & \n",
    "    (col(\"dropoff_longitude\") != 0) & \n",
    "    (col(\"dropoff_latitude\") != 0)\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6739dca",
   "metadata": {},
   "source": [
    "### Data Enrichment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "72510f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the geojson file\n",
    "import json\n",
    "with open('nyc-boroughs.geojson', 'r') as file:\n",
    "    geojson_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee0c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting the geojson data to the workers\n",
    "broadcast_geojson = spark.sparkContext.broadcast(geojson_data)\n",
    "\n",
    "# Verify that the broadcast has been done\n",
    "assert broadcast_geojson.value == geojson_data, \"Broadcasting failed!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "19377a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, shape\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def preprocess_geojson(geojson_boroughs: dict) -> list:\n",
    "    \"\"\"\n",
    "    Preprocesses the geojson data to create a sorted list of polygons.\n",
    "    \n",
    "    Args:\n",
    "    - geojson_boroughs (dict): The geojson data containing polygons and their properties.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A sorted list of tuples where each tuple contains a polygon and its associated properties.\n",
    "    \"\"\"\n",
    "    \n",
    "    polygons_with_props = [\n",
    "        (shape(feature['geometry']), feature['properties'])\n",
    "        for feature in geojson_boroughs['features']\n",
    "    ]\n",
    "    return sorted(polygons_with_props, key=lambda x: (x[1]['boroughCode'], -x[0].area))\n",
    "\n",
    "def get_borough(longitude: float, latitude: float, sorted_polygons: list) -> str:\n",
    "    \"\"\"\n",
    "    Return the borough name for a given longitude and latitude.\n",
    "    \n",
    "    Args:\n",
    "    - longitude (float): Longitude of the location.\n",
    "    - latitude (float): Latitude of the location.\n",
    "    - sorted_polygons (list): List of preprocessed polygons for lookup.\n",
    "    \n",
    "    Returns:\n",
    "    - str: Name of the borough if the point falls within a polygon, otherwise None.\n",
    "    \"\"\"\n",
    "    point = Point(longitude, latitude)\n",
    "    for polygon, properties in sorted_polygons:\n",
    "        if polygon.contains(point):\n",
    "            return properties['borough']\n",
    "    return None\n",
    "\n",
    "# Preprocess polygons and then broadcast\n",
    "sorted_polygons = preprocess_geojson(broadcast_geojson.value)\n",
    "sorted_polygons_broadcast = spark.sparkContext.broadcast(sorted_polygons)\n",
    "\n",
    "# Convert the get_borough function to a UDF using lambda to pass the broadcasted sorted_polygons\n",
    "borough_udf = udf(lambda lon, lat: get_borough(lon, lat, sorted_polygons_broadcast.value), StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "28f571c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 272:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+\n",
      "|pickup_borough|dropoff_borough|\n",
      "+--------------+---------------+\n",
      "|Manhattan     |Manhattan      |\n",
      "|Manhattan     |Manhattan      |\n",
      "|Manhattan     |Manhattan      |\n",
      "|Manhattan     |Manhattan      |\n",
      "|Manhattan     |Manhattan      |\n",
      "+--------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use withColumn for pickup_borough\n",
    "taxi_data_enriched = (taxi_data_final\n",
    "                      .withColumn(\"pickup_borough\", \n",
    "                                  borough_udf(col(\"pickup_longitude\"), col(\"pickup_latitude\"))))\n",
    "\n",
    "# Use withColumn again for dropoff_borough\n",
    "taxi_data_enriched = (taxi_data_enriched\n",
    "                      .withColumn(\"dropoff_borough\", \n",
    "                                  borough_udf(col(\"dropoff_longitude\"), col(\"dropoff_latitude\"))))\n",
    "\n",
    "# Show results\n",
    "taxi_data_enriched.limit(5).select(\"pickup_borough\", \"dropoff_borough\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b37bd07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "taxi_data_enriched = taxi_data_enriched.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "646eaa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|pickup_borough|\n",
      "+--------------+\n",
      "|        Queens|\n",
      "|      Brooklyn|\n",
      "| Staten Island|\n",
      "|          null|\n",
      "|     Manhattan|\n",
      "|         Bronx|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 283:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|dropoff_borough|\n",
      "+---------------+\n",
      "|         Queens|\n",
      "|       Brooklyn|\n",
      "|  Staten Island|\n",
      "|           null|\n",
      "|      Manhattan|\n",
      "|          Bronx|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display distinct pickup boroughs\n",
    "taxi_data_enriched.select(\"pickup_borough\").distinct().show()\n",
    "\n",
    "# Display distinct dropoff boroughs\n",
    "taxi_data_enriched.select(\"dropoff_borough\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b902eacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 293:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with 'null' pickup_borough: 119\n",
      "Number of rows with 'null' dropoff_borough: 373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count rows where pickup_borough is \"null\"\n",
    "pickup_null_count = taxi_data_enriched.filter(taxi_data_enriched[\"pickup_borough\"].isNull()).count()\n",
    "\n",
    "# Count rows where dropoff_borough is \"null\"\n",
    "dropoff_null_count = taxi_data_enriched.filter(taxi_data_enriched[\"dropoff_borough\"].isNull()).count()\n",
    "\n",
    "print(f\"Number of rows with 'null' pickup_borough: {pickup_null_count}\")\n",
    "print(f\"Number of rows with 'null' dropoff_borough: {dropoff_null_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "409c2134",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data_enriched = taxi_data_enriched.filter(\n",
    "    (taxi_data_enriched[\"pickup_borough\"].isNotNull()) & \n",
    "    (taxi_data_enriched[\"dropoff_borough\"].isNotNull())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c7872caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|pickup_borough|\n",
      "+--------------+\n",
      "|        Queens|\n",
      "|      Brooklyn|\n",
      "| Staten Island|\n",
      "|     Manhattan|\n",
      "|         Bronx|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 303:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|dropoff_borough|\n",
      "+---------------+\n",
      "|         Queens|\n",
      "|       Brooklyn|\n",
      "|  Staten Island|\n",
      "|      Manhattan|\n",
      "|          Bronx|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display distinct pickup boroughs\n",
    "taxi_data_enriched.select(\"pickup_borough\").distinct().show()\n",
    "\n",
    "# Display distinct dropoff boroughs\n",
    "taxi_data_enriched.select(\"dropoff_borough\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2b966c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 311:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Shape after cleaning\n",
    "print(taxi_data_enriched.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "391d9ecc",
   "metadata": {},
   "source": [
    "### Query 2: Average Time to Next Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b668e6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:===================================>                 (135 + 12) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+------------------------+\n",
      "|        hack_license|dropoff_borough|avg_waiting_time_seconds|\n",
      "+--------------------+---------------+------------------------+\n",
      "|001C8AAB90AEE49F3...|       Brooklyn|                  5940.0|\n",
      "|001C8AAB90AEE49F3...|      Manhattan|                  3510.0|\n",
      "|0025133AD810DBE80...|      Manhattan|                  2400.0|\n",
      "|002C093A2CB9FD40C...|       Brooklyn|                   450.0|\n",
      "|002C093A2CB9FD40C...|      Manhattan|      1029.2307692307693|\n",
      "|002C093A2CB9FD40C...|         Queens|                  1020.0|\n",
      "|00374328FBA75FBFC...|         Queens|                    null|\n",
      "|00447A6197DBB329F...|       Brooklyn|                  2940.0|\n",
      "|00447A6197DBB329F...|      Manhattan|                  2850.0|\n",
      "|00447A6197DBB329F...|         Queens|                  4800.0|\n",
      "|0046F1E91AA13DEDE...|      Manhattan|       553.3333333333334|\n",
      "|00567B1CBFD51DDFA...|      Manhattan|                   504.0|\n",
      "|0057CCB5BA8D29E34...|      Manhattan|                    null|\n",
      "|006114F940CB87B3A...|      Manhattan|       594.2857142857143|\n",
      "|006114F940CB87B3A...|         Queens|                  3840.0|\n",
      "|006313464EC98A24B...|      Manhattan|                  1575.0|\n",
      "|006B6BD90C7B5C985...|      Manhattan|       686.6666666666666|\n",
      "|00711D0CC3FB5BC90...|       Brooklyn|                 45060.0|\n",
      "|00711D0CC3FB5BC90...|      Manhattan|                  1650.0|\n",
      "|00711D0CC3FB5BC90...|         Queens|                  2700.0|\n",
      "+--------------------+---------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lead, col, avg, when\n",
    "\n",
    "# Define a window specification\n",
    "windowSpec = Window.partitionBy(\"hack_license\").orderBy(\"pickup_timestamp\")\n",
    "\n",
    "# Get the pickup timestamp of the next trip for each taxi's row\n",
    "taxi_data_enriched = taxi_data_enriched.withColumn(\"next_pickup_timestamp\", \n",
    "                                                  lead(\"pickup_timestamp\").over(windowSpec))\n",
    "\n",
    "# Compute the time difference in seconds between dropoff and the next pickup\n",
    "taxi_data_enriched = taxi_data_enriched.withColumn(\"waiting_time\", \n",
    "           when(col(\"next_pickup_timestamp\").isNotNull(), \n",
    "                (col(\"next_pickup_timestamp\").cast(\"long\") - col(\"dropoff_timestamp\").cast(\"long\")))\n",
    "           .otherwise(None))\n",
    "\n",
    "# Compute the average waiting time for each taxi in each dropoff borough\n",
    "average_waiting_time_per_taxi_per_borough = (taxi_data_enriched.groupBy(\"hack_license\", \"dropoff_borough\")\n",
    "                                .agg(avg(\"waiting_time\").alias(\"avg_waiting_time_seconds\"))\n",
    "                                .orderBy(\"hack_license\", \"dropoff_borough\"))\n",
    "\n",
    "#Droping the no needed any longer column\n",
    "taxi_data_enriched = taxi_data_enriched.drop(\"next_pickup_timestamp\", \"waiting_time\")\n",
    "\n",
    "# Show the results\n",
    "average_waiting_time_per_taxi_per_borough.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abe1b94e",
   "metadata": {},
   "source": [
    "### Query 3: Intra-Borough Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "745111b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 313:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|number_of_trips|\n",
      "+---------------+\n",
      "|          85944|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "same_borough_trips = taxi_data_enriched.filter(col(\"pickup_borough\") == col(\"dropoff_borough\"))\n",
    "\n",
    "# Count the number of these trips grouped by borough\n",
    "count_same_borough_trips = same_borough_trips.agg(count(\"*\").alias(\"number_of_trips\"))\\\n",
    "                                             .orderBy(\"number_of_trips\", ascending=False)\n",
    "\n",
    "count_same_borough_trips.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9595cf01",
   "metadata": {},
   "source": [
    "### Query 4: Inter-Borough Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "81c10010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 315:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|number_of_trips|\n",
      "+---------------+\n",
      "|          11431|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "# Filter rows where pickup_borough is not equal to dropoff_borough\n",
    "inter_borough_trips = taxi_data_enriched.filter(col(\"pickup_borough\") != col(\"dropoff_borough\"))\n",
    "\n",
    "# Count the number of these trips grouped by pickup and dropoff boroughs\n",
    "count_inter_borough_trips = inter_borough_trips.agg(count(\"*\").alias(\"number_of_trips\"))\\\n",
    "                                               .orderBy(\"number_of_trips\", ascending=False)\n",
    "\n",
    "count_inter_borough_trips.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d357726",
   "metadata": {},
   "source": [
    "### Query 1 : Utilization: This is per taxi/driver. This can be computed by computing the idle time per taxi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6c7b8dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 317:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-------------------+\n",
      "|        hack_license|dropoff_borough|        utilization|\n",
      "+--------------------+---------------+-------------------+\n",
      "|00BFF9F028A7A9365...|         Queens|                1.0|\n",
      "|00BFF9F028A7A9365...|       Brooklyn| 0.2962962962962963|\n",
      "|00BFF9F028A7A9365...|      Manhattan| 0.3504273504273504|\n",
      "|02856AFC22881ABCA...|      Manhattan|0.41333333333333333|\n",
      "|03A2D28F831C5C3E5...|      Manhattan|0.23809523809523808|\n",
      "+--------------------+---------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag, col, when, unix_timestamp, sum\n",
    "\n",
    "# Define a window specification\n",
    "windowSpec = Window.partitionBy(\"hack_license\").orderBy(\"pickup_timestamp\")\n",
    "\n",
    "# Calculate previous_dropoff_timestamp\n",
    "taxi_data_enriched = taxi_data_enriched.withColumn(\n",
    "    \"previous_dropoff_timestamp\", lag(col(\"dropoff_timestamp\")).over(windowSpec)\n",
    ")\n",
    "\n",
    "# Calculate idle_time_seconds\n",
    "taxi_data_enriched = taxi_data_enriched.withColumn(\n",
    "    \"idle_time_seconds\",\n",
    "    when(col(\"previous_dropoff_timestamp\").isNull(), 0)\n",
    "    .otherwise(\n",
    "        (unix_timestamp(\"pickup_timestamp\") - unix_timestamp(\"previous_dropoff_timestamp\"))\n",
    "    )\n",
    ").withColumn(\n",
    "    \"idle_time_seconds\",\n",
    "    when(col(\"idle_time_seconds\") > 14400, 0)\n",
    "    .otherwise(col(\"idle_time_seconds\"))\n",
    ")\n",
    "\n",
    "# Calculate the duration of each trip in seconds\n",
    "taxi_data_enriched = taxi_data_enriched.withColumn(\n",
    "    \"trip_duration_seconds\", \n",
    "    unix_timestamp(\"dropoff_timestamp\") - unix_timestamp(\"pickup_timestamp\")\n",
    ")\n",
    "\n",
    "# Drop the intermediate column to save memory\n",
    "taxi_data_enriched = taxi_data_enriched.drop(\"previous_dropoff_timestamp\")\n",
    "\n",
    "# Group by taxi and borough, then compute total trip duration and total idle time\n",
    "taxi_borough_utilization = taxi_data_enriched.groupBy(\"hack_license\", \"dropoff_borough\").agg(\n",
    "    sum(\"trip_duration_seconds\").alias(\"total_trip_duration\"),\n",
    "    sum(\"idle_time_seconds\").alias(\"total_idle_time\")\n",
    ")\n",
    "\n",
    "# Calculate the utilization for each taxi within each borough\n",
    "taxi_borough_utilization = taxi_borough_utilization.withColumn(\n",
    "    \"utilization\",\n",
    "    col(\"total_trip_duration\") / (col(\"total_trip_duration\") + col(\"total_idle_time\"))\n",
    ")\n",
    "\n",
    "# Show a sample of the result\n",
    "taxi_borough_utilization.select(\"hack_license\", \"dropoff_borough\", \"utilization\").limit(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9cc11dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 325:===================================================>  (95 + 5) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------+---------+------+-------------+\n",
      "|        hack_license|Bronx|Brooklyn|Manhattan|Queens|Staten Island|\n",
      "+--------------------+-----+--------+---------+------+-------------+\n",
      "|00BFF9F028A7A9365...| null|   29.63|    35.04| 100.0|         null|\n",
      "|02856AFC22881ABCA...| null|    null|    41.33|  null|         null|\n",
      "|03A2D28F831C5C3E5...| null|    null|    23.81| 31.45|         null|\n",
      "|05679B05C691B1A10...| null|    null|    56.25|  null|         null|\n",
      "|069B5562096AF7684...| null|    null|    100.0|  null|         null|\n",
      "|083D8CEB07A9C923F...| null|    null|    37.66| 87.69|         null|\n",
      "|0FBF11956EE14B253...| null|    null|    24.38| 19.47|         null|\n",
      "|1186CE6BC2838695A...| null|   48.15|    27.43|  null|         null|\n",
      "|130328475AD7427AF...| null|    null|     null| 100.0|         null|\n",
      "|138B0A7B7D3B898E4...| null|    null|     null| 100.0|         null|\n",
      "+--------------------+-----+--------+---------+------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, round\n",
    "\n",
    "# Pivot the DataFrame\n",
    "pivot_df = taxi_borough_utilization.groupBy(\"hack_license\").pivot(\"dropoff_borough\").agg(round(first(\"utilization\") * 100, 2))\n",
    "\n",
    "# Show the reshaped DataFrame\n",
    "pivot_df.limit(10).show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebcc8d35",
   "metadata": {},
   "source": [
    "### Analysis of Taxi Utilization\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6450cec2",
   "metadata": {},
   "source": [
    "\n",
    "When analyzing the utilization of taxis, we employ the following formula:\n",
    "\n",
    "$$\n",
    "\\text{Utilization} = \\frac{\\text{Total Trip Duration}}{\\text{Total Trip Duration} + \\text{Idle Time}}\n",
    "$$\n",
    "\n",
    "Given this formula, if a taxi has an **Idle Time** of \\(0\\) (indicating it has not been idle), the equation simplifies to:\n",
    "\n",
    "$$\n",
    "\\text{Utilization} = \\frac{\\text{Total Trip Duration}}{\\text{Total Trip Duration}} = 1\n",
    "$$\n",
    "\n",
    "Expressed as a percentage, this equates to \\(100\\%\\).\n",
    "\n",
    "This means that if a taxi records no idle time, it has been utilized for the entire duration considered, leading to a utilization rate of \\(100\\%\\). This is an essential aspect to consider when interpreting taxis with maximum utilization, as it may hint at continuous service without any downtime and it needs further inspections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42507641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 144:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|        hack_license|num_entries|\n",
      "+--------------------+-----------+\n",
      "|28A7C858D9231A3EC...|          2|\n",
      "|CF1BF49F7229C883D...|          2|\n",
      "|A3E12DBA6882D47DB...|          2|\n",
      "|B3C8637D063490373...|          2|\n",
      "|108AAECE2F72BB70F...|          2|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Extract all hack_license(s) with utilization of 1 (or 100% when expressed as a percentage)\n",
    "licenses_with_100_utilization = pivot_df.filter(col(\"Manhattan\") == 100).select(\"hack_license\").rdd.flatMap(lambda x: x).collect()  \n",
    "# Adjust the borough name as necessary if you are looking at a borough other than Manhattan\n",
    "\n",
    "# Filter the taxi_data_enriched DataFrame for hack_license values with 100% utilization\n",
    "filtered_data = taxi_data_enriched.filter(col(\"hack_license\").isin(licenses_with_100_utilization))\n",
    "\n",
    "# Group by hack_license and count entries\n",
    "license_counts = filtered_data.groupBy(\"hack_license\").agg(count(\"*\").alias(\"num_entries\"))\n",
    "\n",
    "# Filter licenses with more than one entry\n",
    "licenses_with_multiple_entries = license_counts.filter(col(\"num_entries\") > 1)\n",
    "\n",
    "# Show the hack_license values with more than one entry\n",
    "licenses_with_multiple_entries.limit(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "414e2a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------+--------------+---------------+---------------------+------------+-----------------+---------------------+--------------------------+\n",
      "|        hack_license|   pickup_timestamp|  dropoff_timestamp|Duration|pickup_borough|dropoff_borough|next_pickup_timestamp|waiting_time|idle_time_seconds|trip_duration_seconds|previous_dropoff_timestamp|\n",
      "+--------------------+-------------------+-------------------+--------+--------------+---------------+---------------------+------------+-----------------+---------------------+--------------------------+\n",
      "|CF1BF49F7229C883D...|2013-01-13 03:26:00|2013-01-13 03:37:00|    11.0|     Manhattan|      Manhattan|  2013-01-13 03:37:00|           0|                0|                  660|                      null|\n",
      "|CF1BF49F7229C883D...|2013-01-13 03:37:00|2013-01-13 03:41:00|     4.0|     Manhattan|      Manhattan|                 null|        null|                0|                  240|       2013-01-13 03:37:00|\n",
      "+--------------------+-------------------+-------------------+--------+--------------+---------------+---------------------+------------+-----------------+---------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick the second hack_license value from the DataFrame\n",
    "selected_license = licenses_with_multiple_entries.select(\"hack_license\").collect()[1][\"hack_license\"]\n",
    "\n",
    "# Filter the taxi_data_enriched DataFrame for the selected hack_license\n",
    "selected_license_data = taxi_data_enriched.filter(col(\"hack_license\") == selected_license)\n",
    "\n",
    "# Show the details for the selected hack_license\n",
    "selected_license_data.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f57c5d0",
   "metadata": {},
   "source": [
    "> The pickup time is immediately after the dropoff time of the previous ride, indicating that there was no idle time between the two rides."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
